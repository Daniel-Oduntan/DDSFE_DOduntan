{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Daniel-Oduntan/FDSfE_DOduntan/blob/main/HW7/Copy_of_Daniel's_NLP_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w4lqHN1ygnK"
   },
   "source": [
    "# Sentiment Analysis of Movie Reviews:\n",
    "## Data - \n",
    "the data comes from the Kaggla challenge https://www.kaggle.com/code/yagli18/sentiment-analysis-on-movie-reviews and is comprosed of ~10k movie reviews and associated ratings. \n",
    "The challenge there is to create a model that predicts the rating based on the review, which is a Natural Language Processing sentiment analysis task. Sentiment analysis measures the \"sentiment\" (typically in a positive-to-negative linear scale) of a text. There are many issues with sentiment analysis in genera, and in particulat here, the \"sentiment\" of the review (negative, which is associated with sad and angry for example) may not match with the judgement on the quality of the movie (which may be sad, but good if it is a drama) \n",
    "\n",
    "**We will simply apply a pre-trained model that weights each word in the text to measure its negative-to-positive sentiment and compounds all the words to get an overall sentiment. The right way to do this tho would be to _train_ a new model on the data, learning the match between the sentiment of the words and the rating assigned by the author of the review. That way, the model is _specific_ to this data. This is a challenge that I live for you!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlVIQs7CfP2i",
    "outputId": "03f1da73-be61-4a1c-81db-9b6e003da79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWrCMtTrxykK",
    "outputId": "6223c5d1-7df5-4f03-b531-61a6cb1bf85e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting stop_words\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
      "Building wheels for collected packages: stop-words\n",
      "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32912 sha256=885efc36ba6715b2d14cd9d7adae0cdc62c41187a3867e2a00077a205a44bf87\n",
      "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
      "Successfully built stop-words\n",
      "Installing collected packages: stop-words\n",
      "Successfully installed stop-words-2018.7.23\n"
     ]
    }
   ],
   "source": [
    "!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BP_en32qxT-W",
    "outputId": "1c7fe695-467f-495d-90af-dc2d37af426a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 4.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.9.24)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GYIcpS7efhpd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boet5K5Ee8dy",
    "outputId": "0e80365c-9148-4e77-f916-9de91cd824c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "La8elda0fFTD",
    "outputId": "986ce693-0ac3-413f-847c-23e7a9046f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3f4kWejSfgDB"
   },
   "outputs": [],
   "source": [
    "env = json.load(open(\"kaggle.json\", \"r\"))\n",
    "os.environ[\"KAGGLE_USERNAME\"] = env[\"username\"]\n",
    "os.environ[\"KAGGLE_KEY\"] = env[\"key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgq_8NpmfcQ-",
    "outputId": "5da45f95-873c-42ee-b25a-a8e6fbcf17da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: kaggle datasets [-h]\n",
      "                       {list,files,download,create,version,init,metadata,status}\n",
      "                       ...\n",
      "kaggle datasets: error: argument command: invalid choice: 'sentiment' (choose from 'list', 'files', 'download', 'create', 'version', 'init', 'metadata', 'status')\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzRQNgt-o_tz",
    "outputId": "9dcd5826-6c08-4aae-90be-e497130bf411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘sentimentanalysis’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir sentimentanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nfC6axxGpMcc",
    "outputId": "ccee9227-d326-4f0f-fe79-68e7193c9f22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/sentimentanalysis\n"
     ]
    }
   ],
   "source": [
    "cd sentimentanalysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmczZNDQnMut",
    "outputId": "1953c660-deaf-4370-c652-f4f6ad76e93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment-analysis-on-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmZ176TYnanR",
    "outputId": "f3b31272-1c71-4430-b8bd-565dcd6597e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleSubmission.csv\t\t\t test.tsv.zip\n",
      "sentiment-analysis-on-movie-reviews.zip  train.tsv.zip\n"
     ]
    }
   ],
   "source": [
    "!ls *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrzOJTCVpJpb",
    "outputId": "55a9062d-ae27-443b-c719-52542485301c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  sentiment-analysis-on-movie-reviews.zip\n",
      "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip  sentiment-analysis-on-movie-reviews.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbjxZ1r9pZeX"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"train.tsv.zip\", sep=\"\\t\")\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24pjKSBc5yxY"
   },
   "outputs": [],
   "source": [
    "reviews = reviews.groupby(\"SentenceId\").first()\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDpOVqXW6pn5"
   },
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5051zXm7HSuw"
   },
   "outputs": [],
   "source": [
    "reviews[[\"Phrase\", \"Sentiment\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZF-96-DGnNP"
   },
   "outputs": [],
   "source": [
    "print('Number of Reviews/Documents: {}'.format(len(reviews)))\n",
    "\n",
    "print('Corpus Size (words): {}'.format(np.sum([len(phrase.split()) for phrase in reviews[\"Phrase\"].values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cembJ8bCA-Tj"
   },
   "outputs": [],
   "source": [
    "reviews.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVeYuL-UIqbk"
   },
   "outputs": [],
   "source": [
    "#plot sentiment histogram\n",
    "plt.hist(reviews[\"Sentiment\"], bins=[-0.5, 0.5, 1.5, 2.5, 3.5, 4.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TtfouVe9Z5p"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiHp3IY0pxE0"
   },
   "outputs": [],
   "source": [
    "#NLP packs\n",
    "#import nltk  --> used in this notebook\n",
    "#from nltk.tokenize import word_tokenize --> used in this notebook\n",
    "#from nltk.corpus import stopwords\n",
    "#from stop_words import get_stop_words --> used in this notebook\n",
    "#from textblob import TextBlob , Word\n",
    "#import re \n",
    "#import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Toyb3E3_jEf"
   },
   "outputs": [],
   "source": [
    "#remove some characters like new line from the strings\n",
    "reviews['Phrase'] = reviews['Phrase'].str.lower(\n",
    "    ).str.replace(\n",
    "        '[^\\w\\s]', '').str.replace(\n",
    "            '\\d+', '').str.replace(\n",
    "                '\\n',' ').replace(\n",
    "                        '\\r','').str.replace(\n",
    "                            \"[^a-zA-Z0-9\\s]\",'') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvGhYTCUKeCO"
   },
   "source": [
    "# lets talk about the lambda construct and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHskiXRp-JS8"
   },
   "outputs": [],
   "source": [
    "reviews['Phrase'] = reviews['Phrase'].apply(lambda x: word_tokenize(x)) # sentence -> words\n",
    "reviews['Phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HBvme9AvKwX"
   },
   "source": [
    "# lets talk about stop words... and list comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5Jq1PW9-8RB"
   },
   "outputs": [],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R7s3OjW-dOd"
   },
   "outputs": [],
   "source": [
    "reviews[\"Phrase\"] = reviews[\"Phrase\"].apply(lambda x: [word for word in x if word not in STOPWORDS]) # stop words removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SewshuVMLcRO"
   },
   "source": [
    "# Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuF-aPL9LW-t"
   },
   "outputs": [],
   "source": [
    "nltk.FreqDist(reviews.Phrase.sum()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHrWYuX2DM5j"
   },
   "outputs": [],
   "source": [
    "#reviews[\"Phrase\"] = reviews[\"Phrase\"].apply(lambda x : \n",
    "#                  [word for word in x if word not in \n",
    "#                   [\"s\", \"nt\", \"rrb\", \"lrb\", \"one\"]]) # more \"stop\" words \n",
    "# or I could do it by length: are there gpoing to be key words that are <4 characters??\n",
    "reviews[\"Phrase\"] = reviews[\"Phrase\"].apply(lambda x : \n",
    "                  [word for word in x if len(word)>3]) # more \"stop\" words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpWVa_D9DXzB"
   },
   "outputs": [],
   "source": [
    "words_freq = pd.DataFrame(nltk.FreqDist(reviews.Phrase.sum(\n",
    "                                          )).most_common(10), \n",
    "                          columns=['Top Words', 'Frequency'])\n",
    "ax = sns.barplot(x=\"Top Words\",y=\"Frequency\",data=words_freq)\n",
    "plt.xticks(rotation = 45); # Rotates X-Axis Ticks by 45-degrees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4fwBpYSLodb"
   },
   "source": [
    "# Lets talk about the str.join() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr1a-m0u-xus"
   },
   "outputs": [],
   "source": [
    "def wc(data,bgcolor): # word cloud\n",
    "    plt.figure(figsize=(10,10))\n",
    "    wc = WordCloud(background_color=bgcolor, max_words=100, \n",
    "                 min_word_length=4, contour_width=3, \n",
    "                 contour_color='steelblue')\n",
    "    wc.generate(\" \".join(data)) # tum kelimeleri birlestirip tek 1 string haline getirir\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_9ESn1dWJG2"
   },
   "source": [
    "#TASK 1 : \n",
    "Use the function above to plot the word cloud **for Sentiment==0**, comment on the plot to describe how a wordcloud should be read and what are interesting words that appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIjyKsi5AP3p"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#plot the word cloud for sentiment 0\n",
    "wc(reviews[reviews[\"Sentiment\"]==0][\"Phrase\"].sum(), bgcolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-skVAWhfww-0"
   },
   "source": [
    "This figure utilizes a function to look at the first 100 words that are in the Phrase column with a sentiment of zero and creates a word cloud that makes the size of the word proportional to how frequently the word appears. Some interesting words include \"stupid\", \"worst\", \"dull\", and \"predictable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InM6YP9XWek5"
   },
   "source": [
    "#TASK 2 : \n",
    "Use the function above to plot the word cloud **for Sentiment==4**, comment on the plot to describe how a wordcloud should be read and what are interesting words that appear. How does it differ from Sentiment==0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVl6NCgKWjOX"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#plot the word cloud for sentiment 4\n",
    "wc(reviews[reviews[\"Sentiment\"]==4][\"Phrase\"].sum(), bgcolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0szesT_-xA2x"
   },
   "source": [
    "This figure utilizes a function to look at the first 100 words that are in the Phrase column with a sentiment of four and creates a word cloud that makes the size of the word proportional to how frequently the word appears. Some interesting words include \"funny\", \"love\", \"comedy\", and \"entertaining\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UukNpgMaL5Mi"
   },
   "source": [
    "# Task 3: sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KuiRvPWWnzN"
   },
   "source": [
    "install a package that contains a pretrained model: this model will score every word in a sentence as positive or negative (with a continuous score from -1 to 1) and will then average over those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r42wsBvPpz6"
   },
   "outputs": [],
   "source": [
    "#create a model\n",
    "sid_obj = SentimentIntensityAnalyzer() \n",
    "#apply the model to a string\n",
    "sid_obj.polarity_scores(\"series escapades demonstrating adage good goose good gander occasionally amuses none amounts much story\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUzzyBHJYEB9"
   },
   "outputs": [],
   "source": [
    "#define a function to generate the senriment given a string\n",
    "def sentiment_vader(words):\n",
    "  sid_obj = SentimentIntensityAnalyzer()\n",
    "  sentiment_dict = sid_obj.polarity_scores(' '.join(words))\n",
    "  return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV8Q22doUjBb"
   },
   "source": [
    "## TASK steps: \n",
    "1. take the review for each Sentiment column value (you can use Sentiment==0, Sentiment==1..... like you should have done for the wordclouds) \n",
    "2. for each Sentiment group calculate the sentiment of each Phrase, the mean sentiment across the Phrases, and the standard deviation over this mean\n",
    "3. plot on the x axis the value of the Sentiment column [0, 1, 2, 3, 4]\n",
    "4. plot on the y axis the _mean_ sentiment as measured by your code for all reviews with that Sentiment value in the dataset\n",
    "5. include the errorbar on the measured sentiment (use plt.errorbar to do this)\n",
    "6. Comment on the figure: what do you see? is the model good?\n",
    "\n",
    "### hint: you may want to use a list comprehension to do this! the steps are not necessarily sequential: everything can be done with a single line of code. This is not required however"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u9oolqFNobR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swaMgPnAd37I"
   },
   "outputs": [],
   "source": [
    "s0 = reviews[reviews[\"Sentiment\"]==0][\"Phrase\"]\n",
    "s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PL3kY_PofPYK"
   },
   "outputs": [],
   "source": [
    "s1 = reviews[reviews[\"Sentiment\"]==1][\"Phrase\"]\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa6AmOnhfct3"
   },
   "outputs": [],
   "source": [
    "s2 = reviews[reviews[\"Sentiment\"]==2][\"Phrase\"]\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSOgWhLkfhdr"
   },
   "outputs": [],
   "source": [
    "s3 = reviews[reviews[\"Sentiment\"]==3][\"Phrase\"]\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsAWVaxyeEqR"
   },
   "outputs": [],
   "source": [
    "s4 = reviews[reviews[\"Sentiment\"]==4][\"Phrase\"]\n",
    "s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRlnqvUXo3pm"
   },
   "outputs": [],
   "source": [
    "s0mean = st.mean(sentiment_vader(Phrase) for Phrase in s0.values)\n",
    "print(\"The mean of phrases with 0 sentiment = {0:.3f}\".format(s0mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBvA6lyuo7IB"
   },
   "outputs": [],
   "source": [
    "s0std = st.stdev(sentiment_vader(Phrase) for Phrase in s0.values)\n",
    "print(\"The standard deviation of phrases with 0 sentiment = {0:.3f}\".format(s0std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14wkoj4GfZsW"
   },
   "outputs": [],
   "source": [
    "s1mean = st.mean(sentiment_vader(Phrase) for Phrase in s1.values)\n",
    "print(\"The mean of phrases with 1 sentiment = {0:.3f}\".format(s1mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyenFNl_pqWn"
   },
   "outputs": [],
   "source": [
    "s1std = st.stdev(sentiment_vader(Phrase) for Phrase in s1.values)\n",
    "print(\"The standard deviation of phrases with 1 sentiment = {0:.3f}\".format(s1std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmNqvnufogWg"
   },
   "outputs": [],
   "source": [
    "s2mean = st.mean(sentiment_vader(Phrase) for Phrase in s2.values)\n",
    "print(\"The mean of phrases with 2 sentiment = {0:.3f}\".format(s2mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mECIfa3IpxMO"
   },
   "outputs": [],
   "source": [
    "s2std = st.stdev(sentiment_vader(Phrase) for Phrase in s2.values)\n",
    "print(\"The standard deviation of phrases with 2 sentiment = {0:.3f}\".format(s2std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DizcuKIZojon"
   },
   "outputs": [],
   "source": [
    "s3mean = st.mean(sentiment_vader(Phrase) for Phrase in s3.values)\n",
    "print(\"The mean of phrases with 3 sentiment = {0:.3f}\".format(s3mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUybCDpUp0mK"
   },
   "outputs": [],
   "source": [
    "s3std = st.stdev(sentiment_vader(Phrase) for Phrase in s3.values)\n",
    "print(\"The standard deviation of phrases with 3 sentiment = {0:.3f}\".format(s3std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7n6thM6ouyd"
   },
   "outputs": [],
   "source": [
    "s4mean = st.mean(sentiment_vader(Phrase) for Phrase in s4.values)\n",
    "print(\"The mean of phrases with 4 sentiment = {0:.3f}\".format(s4mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yup310qnp2KQ"
   },
   "outputs": [],
   "source": [
    "s4std = st.stdev(sentiment_vader(Phrase) for Phrase in s4.values)\n",
    "print(\"The standard deviation of phrases with 4 sentiment = {0:.3f}\".format(s4std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LrCQq-YtGSb"
   },
   "outputs": [],
   "source": [
    "x = [0, 1, 2, 3, 4]\n",
    "y = [s0mean, s1mean, s2mean, s3mean, s4mean]\n",
    "plt.errorbar(x, y, yerr=[s0std, s1std, s2std, s3std, s4std], ecolor=\"Red\", fmt=\"o\")\n",
    "plt.xlabel(\"Sentiments\")\n",
    "plt.ylabel(\"Mean of Sentiments\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3F_CE1PxbE_"
   },
   "source": [
    "The figure above shows the error bar for sentiment predictions with the sentiment vader package.The significant difference between the data point and the cap length shows that the vadar package is inappropriate for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EvGhYTCUKeCO",
    "6HBvme9AvKwX",
    "SewshuVMLcRO",
    "F_9ESn1dWJG2",
    "InM6YP9XWek5"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
